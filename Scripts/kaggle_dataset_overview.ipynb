{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üçì Last Straw Dataset - Comprehensive Overview\n",
    "\n",
    "This notebook provides a complete guide to the **LAST-Straw** synthetic strawberry dataset. \n",
    "It demonstrates how to:\n",
    "1.  **Download & Extract** the dataset automatically from GitHub Releases (handling split archives).\n",
    "2.  **Visualize** RGB images, Depth maps, and Instance Masks.\n",
    "3.  **Inspect Annotations** (Bounding Boxes, Categories, Attributes).\n",
    "4.  **Visualize Matching** relationships (Strawberry connectivity to Peduncles).\n",
    "\n",
    "**Repository**: [https://github.com/SergKurchev/strawberry_synthetic_dataset](https://github.com/SergKurchev/strawberry_synthetic_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading\n",
    "\n",
    "This block handles the entire setup process. It checks if the dataset exists locally (or in Kaggle Inputs). If not, it automatically downloads the split parts from GitHub Releases, combines them, and extracts the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Configuration\n",
    "GITHUB_TYPE = \"releases\" # or \"raw\" if small enough, but here we use releases for split files\n",
    "VERSION_TAG = \"v1.0\"\n",
    "BASE_URL = f\"https://github.com/SergKurchev/strawberry_synthetic_dataset/releases/download/{VERSION_TAG}\"\n",
    "FILES_TO_DOWNLOAD = [\n",
    "    \"strawberry_dataset.zip.001\",\n",
    "    \"strawberry_dataset.zip.002\",\n",
    "    \"strawberry_dataset.zip.003\"\n",
    "]\n",
    "OUTPUT_ZIP = \"strawberry_dataset.zip\"\n",
    "DATASET_ROOT = Path(\"strawberry_dataset\")\n",
    "\n",
    "def setup_dataset():\n",
    "    # 1. Check for existing dataset (Kaggle Input or Local)\n",
    "    search_paths = [\n",
    "        Path(\"strawberry_dataset\"),\n",
    "        Path(\"dataset/strawberry_dataset\"),\n",
    "        Path(\"/kaggle/input/last-straw-dataset/strawberry_dataset\"),\n",
    "        Path(\"/kaggle/input/strawberry_synthetic_dataset/strawberry_dataset\")\n",
    "    ]\n",
    "    \n",
    "    for p in search_paths:\n",
    "        if p.exists() and (p / \"annotations.json\").exists():\n",
    "            print(f\"‚úÖ Dataset found at: {p}\")\n",
    "            return p\n",
    "\n",
    "    print(\"‚¨áÔ∏è Dataset not found. Downloading from GitHub Releases...\")\n",
    "    \n",
    "    # 2. Download Split Parts\n",
    "    os.makedirs(\"temp_download\", exist_ok=True)\n",
    "    for filename in FILES_TO_DOWNLOAD:\n",
    "        file_path = Path(\"temp_download\") / filename\n",
    "        if not file_path.exists():\n",
    "            url = f\"{BASE_URL}/{filename}\"\n",
    "            print(f\"  Downloading {filename}...\")\n",
    "            try:\n",
    "                r = requests.get(url, stream=True)\n",
    "                r.raise_for_status()\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to download {filename}: {e}\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"  {filename} already exists in temp.\")\n",
    "\n",
    "    # 3. Combine Parts\n",
    "    print(\"üì¶ Combining split archive...\")\n",
    "    with open(OUTPUT_ZIP, 'wb') as outfile:\n",
    "        for filename in FILES_TO_DOWNLOAD:\n",
    "            part_path = Path(\"temp_download\") / filename\n",
    "            with open(part_path, 'rb') as infile:\n",
    "                shutil.copyfileobj(infile, outfile)\n",
    "    \n",
    "    # 4. Extract\n",
    "    print(\"üìÇ Extracting dataset...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(OUTPUT_ZIP, 'r') as zip_ref:\n",
    "            zip_ref.extractall(\".\")\n",
    "        print(\"‚úÖ Extraction complete.\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"‚ùå Error: The combined ZIP file is corrupted.\")\n",
    "        return None\n",
    "\n",
    "    # Cleanup\n",
    "    if os.path.exists(\"temp_download\"): shutil.rmtree(\"temp_download\")\n",
    "    if os.path.exists(OUTPUT_ZIP): os.remove(OUTPUT_ZIP)\n",
    "    \n",
    "    return DATASET_ROOT\n",
    "\n",
    "DATASET_PATH = setup_dataset()\n",
    "if DATASET_PATH is None:\n",
    "    raise RuntimeError(\"Failed to setup dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Metadata\n",
    "\n",
    "Load `annotations.json` which contains COCO-style annotations for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATASET_PATH / \"annotations.json\", 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "images = coco_data['images']\n",
    "annotations = coco_data['annotations']\n",
    "categories = {c['id']: c for c in coco_data['categories']}\n",
    "\n",
    "print(f\"üñºÔ∏è Total Images: {len(images)}\")\n",
    "print(f\"üè∑Ô∏è Total Annotations: {len(annotations)}\")\n",
    "print(\"üìã Categories:\")\n",
    "for cid, cat in categories.items():\n",
    "    print(f\"  {cid}: {cat['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization Helpers\n",
    "\n",
    "Functions to draw bounding boxes, decode depth maps, and visualize relationship lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color palette for visualization\n",
    "COLORS = {\n",
    "    0: (0, 255, 0),      # Ripe (Green in BGR, will convert later)\n",
    "    1: (0, 0, 255),      # Unripe (Red)\n",
    "    2: (0, 165, 255),    # Half-ripe (Orange)\n",
    "    3: (19, 69, 139)     # Peduncle (Brown)\n",
    "}\n",
    "\n",
    "def draw_annotations(img, anns, show_bboxes=True, show_labels=True):\n",
    "    vis = img.copy()\n",
    "    for ann in anns:\n",
    "        cat_id = ann['category_id']\n",
    "        color = COLORS.get(cat_id, (255, 255, 255))\n",
    "        x, y, w, h = [int(v) for v in ann['bbox']]\n",
    "        \n",
    "        if show_bboxes:\n",
    "            cv2.rectangle(vis, (x, y), (x+w, y+h), color, 2)\n",
    "        \n",
    "        if show_labels:\n",
    "            label = categories[cat_id]['name']\n",
    "            cv2.putText(vis, label, (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "    return vis\n",
    "\n",
    "def visualize_matching(img, anns):\n",
    "    vis = img.copy()\n",
    "    # Map instance_id to annotation for quick lookup\n",
    "    id_to_ann = {a['instance_id']: a for a in anns}\n",
    "    \n",
    "    for ann in anns:\n",
    "        # If it's a strawberry and has a parent (peduncle)\n",
    "        if ann['category_id'] in [0, 1, 2] and 'parent_id' in ann and ann['parent_id'] != 0:\n",
    "            parent_id = ann['parent_id']\n",
    "            if parent_id in id_to_ann:\n",
    "                parent = id_to_ann[parent_id]\n",
    "                \n",
    "                # Calculate centers\n",
    "                sx, sy, sw, sh = ann['bbox']\n",
    "                center_s = (int(sx + sw/2), int(sy + sh/2))\n",
    "                \n",
    "                px, py, pw, ph = parent['bbox']\n",
    "                center_p = (int(px + pw/2), int(py + ph/2))\n",
    "                \n",
    "                # Draw line\n",
    "                cv2.line(vis, center_s, center_p, (255, 255, 255), 2)\n",
    "                cv2.circle(vis, center_s, 5, COLORS[ann['category_id']], -1)\n",
    "                cv2.circle(vis, center_p, 5, COLORS[3], -1)\n",
    "\n",
    "    return vis\n",
    "\n",
    "def decode_depth(depth_path):\n",
    "    # Depth is stored as 16-bit PNG (R=High, G=Low byte)\n",
    "    # Formula: depth_mm = (R * 256) + G\n",
    "    if not os.path.exists(depth_path): return None\n",
    "    \n",
    "    img = Image.open(depth_path)\n",
    "    depth_arr = np.array(img)\n",
    "    \n",
    "    if len(depth_arr.shape) == 3: # RGBA/RGB\n",
    "        # Setup for 16-bit decoding from channels\n",
    "        depth_mm = (depth_arr[:,:,0].astype(np.uint16) << 8) | depth_arr[:,:,1].astype(np.uint16)\n",
    "    else:\n",
    "        # Already gray 16-bit\n",
    "        depth_mm = depth_arr\n",
    "        \n",
    "    return depth_mm.astype(np.float32) / 1000.0 # Convert to meters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore Samples\n",
    "\n",
    "Let's visualize a few random samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "SAMPLES_TO_SHOW = 3\n",
    "indices = random.sample(range(len(images)), SAMPLES_TO_SHOW)\n",
    "\n",
    "for idx in indices:\n",
    "    img_info = images[idx]\n",
    "    img_path = DATASET_PATH / \"images\" / img_info['file_name']\n",
    "    depth_path = DATASET_PATH / \"depth\" / img_info['file_name']\n",
    "    \n",
    "    # Read Images\n",
    "    rgb = cv2.imread(str(img_path))\n",
    "    rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "    depth_map = decode_depth(depth_path)\n",
    "    \n",
    "    # Get Annotations for this image\n",
    "    img_anns = [a for a in annotations if a['image_id'] == img_info['id']]\n",
    "    \n",
    "    # Create Visualizations\n",
    "    vis_bbox = draw_annotations(rgb, img_anns)\n",
    "    vis_match = visualize_matching(rgb, img_anns)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    ax[0].imshow(vis_bbox)\n",
    "    ax[0].set_title(f\"Annotations: {img_info['file_name']}\")\n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    if depth_map is not None:\n",
    "        im1 = ax[1].imshow(depth_map, cmap='magma')\n",
    "        ax[1].set_title(\"Depth Map (Meters)\")\n",
    "        plt.colorbar(im1, ax=ax[1])\n",
    "    else:\n",
    "        ax[1].text(0.5, 0.5, \"Depth Missing\", ha='center')\n",
    "    ax[1].axis('off')\n",
    "    \n",
    "    ax[2].imshow(vis_match)\n",
    "    ax[2].set_title(\"Matching (Strawberry -> Peduncle)\")\n",
    "    ax[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}