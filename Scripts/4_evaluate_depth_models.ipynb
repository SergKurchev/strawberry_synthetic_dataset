{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 4: Depth Estimation Model Evaluation\n",
                "\n",
                "This notebook evaluates two state-of-the-art depth estimation models:\n",
                "1. **Depth Anything 3** (ByteDance)\n",
                "2. **UniDepthV2** (ETH Zurich)\n",
                "\n",
                "**Goal**: Test absolute depth estimation (in meters) on strawberry dataset\n",
                "**Environment**: Kaggle with GPU support (carefully managed dependencies)\n",
                "\n",
                "> ⚠️ **IMPORTANT: GPU REQUIRED**\n",
                "> These models require a GPU to run efficiently. On Kaggle, please ensure you have enabled **GPU T4 x2** or **P100** in the Accelerator settings.\n",
                "> \n",
                "> **Local CPU Testing**: Not supported for these models due to heavy dependencies (xformers) and high VRAM usage. The notebook will gracefully skip models if they cannot be loaded."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Setup and Dependency Management"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check system info\n",
                "import sys\n",
                "import torch\n",
                "print(f\"Python version: {sys.version}\")\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"CUDA version: {torch.version.cuda}\")\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Install Common Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install common dependencies first\n",
                "!pip install -q opencv-python-headless matplotlib seaborn scikit-learn\n",
                "!pip install -q pillow numpy pandas tqdm einops timm\n",
                "!pip install -q huggingface_hub"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Download Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "from PIL import Image\n",
                "from tqdm.auto import tqdm\n",
                "import cv2\n",
                "\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (15, 10)\n",
                "\n",
                "# Clone dataset repository\n",
                "REPO_URL = \"https://github.com/SergKurchev/strawberry_synthetic_dataset.git\"\n",
                "DATASET_DIR = \"/kaggle/working/strawberry_dataset\"\n",
                "\n",
                "if not os.path.exists(DATASET_DIR):\n",
                "    print(\"Cloning dataset repository...\")\n",
                "    !git clone {REPO_URL} {DATASET_DIR}\n",
                "else:\n",
                "    print(\"Dataset already exists\")\n",
                "\n",
                "# Load metadata\n",
                "with open(os.path.join(DATASET_DIR, \"depth_metadata.json\"), 'r') as f:\n",
                "    depth_metadata = json.load(f)\n",
                "\n",
                "print(f\"\\nDataset loaded: {len(depth_metadata)} images with depth ground truth\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select test images\n",
                "test_images = sorted(list(depth_metadata.keys()))[:20]  # Use first 20 images\n",
                "print(f\"Selected {len(test_images)} test images\")\n",
                "\n",
                "def load_depth_png(depth_path):\n",
                "    \"\"\"Load depth from PNG file with proper 16-bit decoding.\"\"\"\n",
                "    # The depth is encoded as 16-bit value split across R and G channels:\n",
                "    # - R channel: high byte (depth_mm >> 8)\n",
                "    # - G channel: low byte (depth_mm & 0xFF)\n",
                "    img = Image.open(depth_path)\n",
                "    depth_arr = np.array(img)\n",
                "    \n",
                "    if len(depth_arr.shape) == 3 and depth_arr.shape[2] >= 2:\n",
                "        # RGB encoded - 16-bit value in R (high) and G (low) channels\n",
                "        high = depth_arr[:, :, 0].astype(np.uint16)\n",
                "        low = depth_arr[:, :, 1].astype(np.uint16)\n",
                "        depth_mm = (high << 8) | low  # Reconstruct 16-bit value\n",
                "        depth_m = depth_mm.astype(np.float32) / 1000.0  # mm to m\n",
                "    else:\n",
                "        # Single channel - assume already in mm\n",
                "        depth_m = depth_arr.astype(np.float32) / 1000.0\n",
                "    \n",
                "    return depth_m\n",
                "\n",
                "# Visualize sample images with ground truth depth\n",
                "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for idx, img_name in enumerate(test_images[:4]):\n",
                "    # Load RGB\n",
                "    rgb_path = os.path.join(DATASET_DIR, \"images\", img_name)\n",
                "    rgb = cv2.imread(rgb_path)\n",
                "    rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
                "    \n",
                "    # Load depth\n",
                "    depth_path = os.path.join(DATASET_DIR, \"depth\", img_name)\n",
                "    depth_meters = load_depth_png(depth_path)\n",
                "    \n",
                "    # Display RGB\n",
                "    axes[idx*2].imshow(rgb)\n",
                "    axes[idx*2].set_title(f\"RGB: {img_name}\", fontsize=10)\n",
                "    axes[idx*2].axis('off')\n",
                "    \n",
                "    # Display depth\n",
                "    im = axes[idx*2+1].imshow(depth_meters, cmap='turbo', vmin=0, vmax=3)\n",
                "    axes[idx*2+1].set_title(f\"Ground Truth Depth (m)\", fontsize=10)\n",
                "    axes[idx*2+1].axis('off')\n",
                "    plt.colorbar(im, ax=axes[idx*2+1], fraction=0.046)\n",
                "\n",
                "plt.suptitle('Sample Images with Ground Truth Depth', fontsize=16, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.savefig('/kaggle/working/dataset_samples.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model 1: Depth Anything 3\n",
                "\n",
                "### Installation Strategy for Kaggle"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install Depth Anything 3 dependencies\n",
                "print(\"Installing Depth Anything 3...\")\n",
                "\n",
                "# Install xformers (required for DA3)\n",
                "!pip install -q xformers\n",
                "\n",
                "# Clone repository\n",
                "DA3_DIR = \"/kaggle/working/Depth-Anything-3\"\n",
                "if not os.path.exists(DA3_DIR):\n",
                "    !git clone https://github.com/ByteDance-Seed/Depth-Anything-3.git {DA3_DIR}\n",
                "\n",
                "# Install package\n",
                "import sys\n",
                "sys.path.insert(0, DA3_DIR)\n",
                "\n",
                "# Install DA3 in editable mode\n",
                "!cd {DA3_DIR} && pip install -q -e .\n",
                "\n",
                "print(\"Depth Anything 3 installed successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Depth Anything 3 model\n",
                "try:\n",
                "    from depth_anything_3.api import DepthAnything3\n",
                "    \n",
                "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "    \n",
                "    # Load model (using smaller variant for Kaggle)\n",
                "    print(\"Loading Depth Anything 3 model...\")\n",
                "    da3_model = DepthAnything3.from_pretrained(\"depth-anything/DA3NESTED-GIANT-LARGE\")\n",
                "    da3_model = da3_model.to(device=device)\n",
                "    da3_model.eval()\n",
                "    \n",
                "    print(f\"✓ Depth Anything 3 loaded on {device}\")\n",
                "    DA3_AVAILABLE = True\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"⚠ Failed to load Depth Anything 3: {e}\")\n",
                "    print(\"Continuing with UniDepthV2 only...\")\n",
                "    DA3_AVAILABLE = False"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Test Depth Anything 3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if DA3_AVAILABLE:\n",
                "    # Run inference on test images\n",
                "    da3_results = {}\n",
                "    \n",
                "    print(\"Running Depth Anything 3 inference...\")\n",
                "    for img_name in tqdm(test_images):\n",
                "        img_path = os.path.join(DATASET_DIR, \"images\", img_name)\n",
                "        \n",
                "        try:\n",
                "            # Run inference\n",
                "            prediction = da3_model.inference([img_path])\n",
                "            \n",
                "            # Extract depth map\n",
                "            depth_pred = prediction.depth[0]  # [H, W]\n",
                "            \n",
                "            # Store result\n",
                "            da3_results[img_name] = depth_pred\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"Error processing {img_name}: {e}\")\n",
                "            continue\n",
                "    \n",
                "    print(f\"\\n✓ Depth Anything 3 processed {len(da3_results)} images\")\n",
                "else:\n",
                "    print(\"Skipping Depth Anything 3 (not available)\")\n",
                "    da3_results = {}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model 2: UniDepthV2\n",
                "\n",
                "### Installation Strategy for Kaggle"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install UniDepthV2 dependencies\n",
                "print(\"Installing UniDepthV2...\")\n",
                "\n",
                "# Clone repository\n",
                "UNIDEPTH_DIR = \"/kaggle/working/UniDepth\"\n",
                "if not os.path.exists(UNIDEPTH_DIR):\n",
                "    !git clone https://github.com/lpiccinelli-eth/UniDepth.git {UNIDEPTH_DIR}\n",
                "\n",
                "# Install package\n",
                "sys.path.insert(0, UNIDEPTH_DIR)\n",
                "\n",
                "# Install dependencies from requirements\n",
                "!cd {UNIDEPTH_DIR} && pip install -q -e .\n",
                "\n",
                "print(\"UniDepthV2 installed successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load UniDepthV2 model\n",
                "try:\n",
                "    from unidepth.models import UniDepthV2\n",
                "    import torch.nn.functional as F\n",
                "    \n",
                "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "    \n",
                "    # Load model\n",
                "    print(\"Loading UniDepthV2 model...\")\n",
                "    unidepth_model = UniDepthV2.from_pretrained(\"lpiccinelli/unidepth-v2-vitl14\")\n",
                "    unidepth_model = unidepth_model.to(device)\n",
                "    unidepth_model.eval()\n",
                "    \n",
                "    print(f\"✓ UniDepthV2 loaded on {device}\")\n",
                "    UNIDEPTH_AVAILABLE = True\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"⚠ Failed to load UniDepthV2: {e}\")\n",
                "    print(\"Trying alternative loading method...\")\n",
                "    \n",
                "    try:\n",
                "        # Alternative: use torch.hub\n",
                "        unidepth_model = torch.hub.load(\"lpiccinelli-eth/UniDepth\", \"UniDepthV2\", trust_repo=True)\n",
                "        unidepth_model = unidepth_model.to(device)\n",
                "        unidepth_model.eval()\n",
                "        print(f\"✓ UniDepthV2 loaded via torch.hub on {device}\")\n",
                "        UNIDEPTH_AVAILABLE = True\n",
                "    except Exception as e2:\n",
                "        print(f\"⚠ Failed to load UniDepthV2: {e2}\")\n",
                "        UNIDEPTH_AVAILABLE = False"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Test UniDepthV2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if UNIDEPTH_AVAILABLE:\n",
                "    # Run inference on test images\n",
                "    unidepth_results = {}\n",
                "    \n",
                "    print(\"Running UniDepthV2 inference...\")\n",
                "    for img_name in tqdm(test_images):\n",
                "        img_path = os.path.join(DATASET_DIR, \"images\", img_name)\n",
                "        \n",
                "        try:\n",
                "            # Load and preprocess image\n",
                "            rgb = Image.open(img_path).convert('RGB')\n",
                "            \n",
                "            # Run inference\n",
                "            with torch.no_grad():\n",
                "                predictions = unidepth_model.infer(rgb)\n",
                "            \n",
                "            # Extract depth map (in meters)\n",
                "            depth_pred = predictions['depth'].cpu().numpy().squeeze()\n",
                "            \n",
                "            # Store result\n",
                "            unidepth_results[img_name] = depth_pred\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"Error processing {img_name}: {e}\")\n",
                "            continue\n",
                "    \n",
                "    print(f\"\\n✓ UniDepthV2 processed {len(unidepth_results)} images\")\n",
                "else:\n",
                "    print(\"Skipping UniDepthV2 (not available)\")\n",
                "    unidepth_results = {}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Evaluation Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define evaluation metrics\n",
                "def compute_depth_metrics(pred, gt, mask=None):\n",
                "    \"\"\"\n",
                "    Compute depth estimation metrics\n",
                "    pred: predicted depth (meters)\n",
                "    gt: ground truth depth (meters)\n",
                "    mask: valid pixels mask\n",
                "    \"\"\"\n",
                "    if mask is None:\n",
                "        mask = (gt > 0) & (gt < 10)  # Valid depth range\n",
                "    \n",
                "    pred = pred[mask]\n",
                "    gt = gt[mask]\n",
                "    \n",
                "    # Align scale (for relative depth models)\n",
                "    scale = np.median(gt) / np.median(pred)\n",
                "    pred_scaled = pred * scale\n",
                "    \n",
                "    # Absolute metrics\n",
                "    abs_rel = np.mean(np.abs(pred_scaled - gt) / gt)\n",
                "    sq_rel = np.mean(((pred_scaled - gt) ** 2) / gt)\n",
                "    rmse = np.sqrt(np.mean((pred_scaled - gt) ** 2))\n",
                "    rmse_log = np.sqrt(np.mean((np.log(pred_scaled) - np.log(gt)) ** 2))\n",
                "    \n",
                "    # Threshold accuracy\n",
                "    thresh = np.maximum((gt / pred_scaled), (pred_scaled / gt))\n",
                "    a1 = (thresh < 1.25).mean()\n",
                "    a2 = (thresh < 1.25 ** 2).mean()\n",
                "    a3 = (thresh < 1.25 ** 3).mean()\n",
                "    \n",
                "    return {\n",
                "        'abs_rel': abs_rel,\n",
                "        'sq_rel': sq_rel,\n",
                "        'rmse': rmse,\n",
                "        'rmse_log': rmse_log,\n",
                "        'a1': a1,\n",
                "        'a2': a2,\n",
                "        'a3': a3,\n",
                "        'scale': scale\n",
                "    }\n",
                "\n",
                "print(\"Evaluation metrics defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Compare Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate both models\n",
                "da3_metrics_list = []\n",
                "unidepth_metrics_list = []\n",
                "\n",
                "print(\"Evaluating models...\\n\")\n",
                "\n",
                "for img_name in test_images:\n",
                "    # Load ground truth\n",
                "    depth_path = os.path.join(DATASET_DIR, \"depth\", img_name)\n",
                "    gt_depth = load_depth_png(depth_path)\n",
                "    \n",
                "    # Evaluate DA3\n",
                "    if img_name in da3_results:\n",
                "        pred_depth = da3_results[img_name]\n",
                "        # Resize if needed\n",
                "        if pred_depth.shape != gt_depth.shape:\n",
                "            pred_depth = cv2.resize(pred_depth, (gt_depth.shape[1], gt_depth.shape[0]))\n",
                "        metrics = compute_depth_metrics(pred_depth, gt_depth)\n",
                "        da3_metrics_list.append(metrics)\n",
                "    \n",
                "    # Evaluate UniDepth\n",
                "    if img_name in unidepth_results:\n",
                "        pred_depth = unidepth_results[img_name]\n",
                "        # Resize if needed\n",
                "        if pred_depth.shape != gt_depth.shape:\n",
                "            pred_depth = cv2.resize(pred_depth, (gt_depth.shape[1], gt_depth.shape[0]))\n",
                "        metrics = compute_depth_metrics(pred_depth, gt_depth)\n",
                "        unidepth_metrics_list.append(metrics)\n",
                "\n",
                "# Aggregate metrics\n",
                "def aggregate_metrics(metrics_list):\n",
                "    if not metrics_list:\n",
                "        return None\n",
                "    \n",
                "    aggregated = {}\n",
                "    for key in metrics_list[0].keys():\n",
                "        if key != 'scale':\n",
                "            aggregated[key] = np.mean([m[key] for m in metrics_list])\n",
                "    return aggregated\n",
                "\n",
                "da3_avg_metrics = aggregate_metrics(da3_metrics_list)\n",
                "unidepth_avg_metrics = aggregate_metrics(unidepth_metrics_list)\n",
                "\n",
                "print(\"\\n=== Depth Anything 3 Results ===\")\n",
                "if da3_avg_metrics:\n",
                "    for key, value in da3_avg_metrics.items():\n",
                "        print(f\"{key}: {value:.4f}\")\n",
                "else:\n",
                "    print(\"Not available\")\n",
                "\n",
                "print(\"\\n=== UniDepthV2 Results ===\")\n",
                "if unidepth_avg_metrics:\n",
                "    for key, value in unidepth_avg_metrics.items():\n",
                "        print(f\"{key}: {value:.4f}\")\n",
                "else:\n",
                "    print(\"Not available\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize comparison\n",
                "if da3_avg_metrics and unidepth_avg_metrics:\n",
                "    metrics_names = ['abs_rel', 'rmse', 'rmse_log', 'a1', 'a2', 'a3']\n",
                "    da3_values = [da3_avg_metrics[m] for m in metrics_names]\n",
                "    unidepth_values = [unidepth_avg_metrics[m] for m in metrics_names]\n",
                "    \n",
                "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
                "    axes = axes.flatten()\n",
                "    \n",
                "    for idx, metric in enumerate(metrics_names):\n",
                "        axes[idx].bar(['DA3', 'UniDepthV2'], [da3_values[idx], unidepth_values[idx]], \n",
                "                     color=['#FF6B6B', '#4ECDC4'])\n",
                "        axes[idx].set_title(metric.upper(), fontweight='bold')\n",
                "        axes[idx].set_ylabel('Value')\n",
                "        axes[idx].grid(True, alpha=0.3)\n",
                "    \n",
                "    plt.suptitle('Model Comparison on Strawberry Dataset', fontsize=16, fontweight='bold')\n",
                "    plt.tight_layout()\n",
                "    plt.savefig('/kaggle/working/model_comparison.png', dpi=150, bbox_inches='tight')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Visual Comparison on Sample Images"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize predictions on 4 sample images\n",
                "sample_images = test_images[:4]\n",
                "\n",
                "fig, axes = plt.subplots(4, 4, figsize=(20, 20))\n",
                "\n",
                "for row, img_name in enumerate(sample_images):\n",
                "    # Load RGB\n",
                "    rgb_path = os.path.join(DATASET_DIR, \"images\", img_name)\n",
                "    rgb = cv2.imread(rgb_path)\n",
                "    rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
                "    \n",
                "    # Load GT depth\n",
                "    depth_path = os.path.join(DATASET_DIR, \"depth\", img_name)\n",
                "    gt_depth = load_depth_png(depth_path)\n",
                "    \n",
                "    # RGB\n",
                "    axes[row, 0].imshow(rgb)\n",
                "    axes[row, 0].set_title('RGB', fontweight='bold')\n",
                "    axes[row, 0].axis('off')\n",
                "    \n",
                "    # Ground Truth\n",
                "    im1 = axes[row, 1].imshow(gt_depth, cmap='turbo', vmin=0, vmax=3)\n",
                "    axes[row, 1].set_title('Ground Truth', fontweight='bold')\n",
                "    axes[row, 1].axis('off')\n",
                "    plt.colorbar(im1, ax=axes[row, 1], fraction=0.046)\n",
                "    \n",
                "    # DA3 prediction\n",
                "    if img_name in da3_results:\n",
                "        pred = da3_results[img_name]\n",
                "        if pred.shape != gt_depth.shape:\n",
                "            pred = cv2.resize(pred, (gt_depth.shape[1], gt_depth.shape[0]))\n",
                "        im2 = axes[row, 2].imshow(pred, cmap='turbo', vmin=0, vmax=3)\n",
                "        axes[row, 2].set_title('Depth Anything 3', fontweight='bold')\n",
                "        axes[row, 2].axis('off')\n",
                "        plt.colorbar(im2, ax=axes[row, 2], fraction=0.046)\n",
                "    else:\n",
                "        axes[row, 2].text(0.5, 0.5, 'N/A', ha='center', va='center', fontsize=20)\n",
                "        axes[row, 2].axis('off')\n",
                "    \n",
                "    # UniDepth prediction\n",
                "    if img_name in unidepth_results:\n",
                "        pred = unidepth_results[img_name]\n",
                "        if pred.shape != gt_depth.shape:\n",
                "            pred = cv2.resize(pred, (gt_depth.shape[1], gt_depth.shape[0]))\n",
                "        im3 = axes[row, 3].imshow(pred, cmap='turbo', vmin=0, vmax=3)\n",
                "        axes[row, 3].set_title('UniDepthV2', fontweight='bold')\n",
                "        axes[row, 3].axis('off')\n",
                "        plt.colorbar(im3, ax=axes[row, 3], fraction=0.046)\n",
                "    else:\n",
                "        axes[row, 3].text(0.5, 0.5, 'N/A', ha='center', va='center', fontsize=20)\n",
                "        axes[row, 3].axis('off')\n",
                "\n",
                "plt.suptitle('Depth Estimation Comparison', fontsize=18, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.savefig('/kaggle/working/visual_comparison.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save comprehensive summary\n",
                "summary = {\n",
                "    \"dataset\": {\n",
                "        \"name\": \"Strawberry Synthetic Dataset\",\n",
                "        \"test_images\": len(test_images),\n",
                "        \"depth_range_meters\": \"0.3 - 2.5\"\n",
                "    },\n",
                "    \"models\": {\n",
                "        \"depth_anything_3\": {\n",
                "            \"available\": DA3_AVAILABLE,\n",
                "            \"metrics\": da3_avg_metrics if da3_avg_metrics else \"N/A\"\n",
                "        },\n",
                "        \"unidepth_v2\": {\n",
                "            \"available\": UNIDEPTH_AVAILABLE,\n",
                "            \"metrics\": unidepth_avg_metrics if unidepth_avg_metrics else \"N/A\"\n",
                "        }\n",
                "    },\n",
                "    \"notes\": [\n",
                "        \"Metrics computed on absolute depth (meters)\",\n",
                "        \"Scale alignment applied using median scaling\",\n",
                "        \"Valid depth range: 0-10 meters\"\n",
                "    ]\n",
                "}\n",
                "\n",
                "with open('/kaggle/working/depth_evaluation_summary.json', 'w') as f:\n",
                "    json.dump(summary, f, indent=2)\n",
                "\n",
                "print(\"\\n=== Evaluation Summary ===\")\n",
                "print(json.dumps(summary, indent=2))\n",
                "\n",
                "print(\"\\n✓ All results saved to /kaggle/working/\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}