{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udf53 Task 6: Depth Anything V2 Evaluation\n",
    "\n",
    "This notebook evaluates the **Depth Anything V2** (Metric Depth) model on the Strawberry Synthetic Dataset.\n",
    "\n",
    "**Key Features:**\n",
    "1.  **Data Load**: Downloads dataset using GitHub Releases (Robust).\n",
    "2.  **Precision**: Uses **`.npy`** files for high-precision Ground Truth depth.\n",
    "3.  **Metric Depth**: Uses the official `metric_depth` model for absolute depth estimation.\n",
    "4.  **Console Inference**: Runs inference via command line script as requested.\n",
    "5.  **Evaluation**: Computes 5 key depth estimation metrics.\n",
    "\n",
    "**Metrics:**\n",
    "- **Abs Rel**: Absolute Relative Error (smaller is better)\n",
    "- **RMSE**: Root Mean Squared Error (smaller is better)\n",
    "- **RMSE log**: Log-space RMSE (smaller is better)\n",
    "- **\u03b41**: Accuracy < 1.25 (larger is better, max 1.0)\n",
    "- **Sq Rel**: Squared Relative Error (smaller is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install Dependencies\n",
    "!git clone https://github.com/DepthAnything/Depth-Anything-V2.git\n",
    "!pip install -r Depth-Anything-V2/requirements.txt\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "import glob\n",
    "import inspect\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# --- Robust Dataset Configuration ---\n",
    "VERSION_TAG = \"Dataset\"\n",
    "BASE_URL = f\"https://github.com/SergKurchev/strawberry_synthetic_dataset/releases/download/{VERSION_TAG}\"\n",
    "FILES_TO_DOWNLOAD = [\n",
    "    \"strawberry_dataset.zip.001\",\n",
    "    \"strawberry_dataset.zip.002\",\n",
    "    \"strawberry_dataset.zip.003\"\n",
    "]\n",
    "OUTPUT_ZIP = \"strawberry_dataset.zip\"\n",
    "\n",
    "def reconstruct_metadata(dataset_root):\n",
    "    \"\"\"Reconstructs depth_metadata.json from individual files in metadata_temp/\"\"\"\n",
    "    print(\"\u26a0\ufe0f 'depth_metadata.json' not found. Attempting reconstruction from 'metadata_temp/'...\")\n",
    "    temp_dir = dataset_root / \"metadata_temp\"\n",
    "    if not temp_dir.exists():\n",
    "        print(f\"\u274c metadata_temp directory not found at {temp_dir}\")\n",
    "        return False\n",
    "\n",
    "    combined_metadata = {}\n",
    "    json_files = list(temp_dir.glob(\"*_meta.json\"))\n",
    "    print(f\"  Found {len(json_files)} metadata chunks.\")\n",
    "    \n",
    "    for json_file in tqdm(json_files, desc=\"Reconstructing Metadata\"):\n",
    "        try:\n",
    "            # Filename format: 00001_meta.json -> corresponds to 00001.png\n",
    "            # We assume the content of the json is the metadata dict for that image\n",
    "            img_id = json_file.name.replace(\"_meta.json\", \"\")\n",
    "            img_name = f\"{img_id}.png\"\n",
    "            \n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                combined_metadata[img_name] = data\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Failed to read {json_file}: {e}\")\n",
    "\n",
    "    if not combined_metadata:\n",
    "        print(\"\u274c Failed to reconstruct any metadata.\")\n",
    "        return False\n",
    "\n",
    "    target_path = dataset_root / \"depth_metadata.json\"\n",
    "    print(f\"\ud83d\udcbe Saving reconstructed metadata to {target_path}...\")\n",
    "    with open(target_path, 'w') as f:\n",
    "        json.dump(combined_metadata, f, indent=2)\n",
    "        \n",
    "    return True\n",
    "\n",
    "def setup_dataset():\n",
    "    # 1. Search for existing dataset\n",
    "    print(\"\ud83d\udd0d Searching for existing dataset...\")\n",
    "    \n",
    "    # Helper to validate a root candidate\n",
    "    def validate_root(p):\n",
    "        if (p / \"depth_metadata.json\").exists():\n",
    "            return True\n",
    "        if (p / \"metadata_temp\").exists():\n",
    "            # Try to fix it\n",
    "            return reconstruct_metadata(p)\n",
    "        return False\n",
    "\n",
    "    # Recursive search in current dir\n",
    "    for root, dirs, files in os.walk(\".\", topdown=True):\n",
    "        p = Path(root)\n",
    "        # Start optimization: don't go too deep or into hidden dirs\n",
    "        if \".git\" in p.parts or \"temp_download\" in p.parts:\n",
    "            continue\n",
    "            \n",
    "        if \"images\" in dirs and (\"depth_metadata.json\" in files or \"metadata_temp\" in dirs):\n",
    "            if validate_root(p):\n",
    "                print(f\"\u2705 Dataset found/Fixed at: {p}\")\n",
    "                return p\n",
    "\n",
    "    # Check standard paths\n",
    "    search_paths = [\n",
    "        Path(\"strawberry_dataset\"),\n",
    "        Path(\"dataset/strawberry_dataset\"),\n",
    "        Path(\"/kaggle/input/last-straw-dataset/strawberry_dataset\"),\n",
    "        Path(\"/kaggle/input/strawberry_synthetic_dataset/strawberry_dataset\")\n",
    "    ]\n",
    "    for p in search_paths:\n",
    "        if p.exists():\n",
    "            if validate_root(p):\n",
    "                print(f\"\u2705 Dataset found/Fixed at: {p}\")\n",
    "                return p\n",
    "\n",
    "    print(\"\u2b07\ufe0f Dataset not found. Downloading from GitHub Releases...\")\n",
    "    \n",
    "    # 2. Prepare Download Directory\n",
    "    if os.path.exists(\"temp_download\"):\n",
    "        shutil.rmtree(\"temp_download\")\n",
    "    os.makedirs(\"temp_download\", exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(OUTPUT_ZIP):\n",
    "        os.remove(OUTPUT_ZIP)\n",
    "\n",
    "    # 3. Download and Combine\n",
    "    with open(OUTPUT_ZIP, 'wb') as outfile:\n",
    "        for filename in FILES_TO_DOWNLOAD:\n",
    "            file_path = Path(\"temp_download\") / filename\n",
    "            url = f\"{BASE_URL}/{filename}\"\n",
    "            \n",
    "            print(f\"  Downloading {filename} from {url}...\")\n",
    "            r = requests.get(url, stream=True)\n",
    "            if r.status_code != 200:\n",
    "                raise RuntimeError(f\"Download failed for {filename}: HTTP {r.status_code}\")\n",
    "            \n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            \n",
    "            file_size_mb = file_path.stat().st_size / 1024 / 1024\n",
    "            print(f\"  Downloaded {filename} ({file_size_mb:.2f} MB). Appending to zip...\")\n",
    "            \n",
    "            with open(file_path, 'rb') as infile:\n",
    "                shutil.copyfileobj(infile, outfile)\n",
    "\n",
    "    # 4. Extract\n",
    "    total_size_mb = os.path.getsize(OUTPUT_ZIP)/1024/1024\n",
    "    print(f\"\ud83d\udcc2 Extracting {OUTPUT_ZIP} ({total_size_mb:.2f} MB)...\")\n",
    "    \n",
    "    try:\n",
    "        with zipfile.ZipFile(OUTPUT_ZIP, 'r') as zip_ref:\n",
    "            zip_ref.extractall(\".\")\n",
    "            print(\"  Extraction complete.\")\n",
    "    except zipfile.BadZipFile as e:\n",
    "        print(f\"\u274c BadZipFile Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "    shutil.rmtree(\"temp_download\", ignore_errors=True)\n",
    "    if os.path.exists(OUTPUT_ZIP):\n",
    "        os.remove(OUTPUT_ZIP)\n",
    "\n",
    "    # --- FIX: Handle potential backslash filenames on Linux ---\n",
    "    print(\"\ud83e\uddf9 Checking for backslash issues in filenames...\")\n",
    "    count = 0\n",
    "    # Iterate over files in current directory to check for backslashes in names\n",
    "    for filename in os.listdir(\".\"):\n",
    "        if \"\\\\\" in filename:\n",
    "            # It's a file with backslashes in name, implying flattened structure\n",
    "            new_path = filename.replace(\"\\\\\", \"/\") # standardize to forward slash\n",
    "            \n",
    "            # Create parent dirs\n",
    "            parent = os.path.dirname(new_path)\n",
    "            if parent:\n",
    "                os.makedirs(parent, exist_ok=True)\n",
    "            \n",
    "            # Move file\n",
    "            try:\n",
    "                shutil.move(filename, new_path)\n",
    "                count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  Failed to move {filename} -> {new_path}: {e}\")\n",
    "            \n",
    "    if count > 0:\n",
    "        print(f\"\u2705 Fixed {count} filenames with backslashes. Directory structure restored.\")\n",
    "        \n",
    "    # 5. Locate and Fix\n",
    "    print(\"\ud83d\udd0e Locating dataset root...\")\n",
    "    for root, dirs, files in os.walk(\".\", topdown=True):\n",
    "        p = Path(root)\n",
    "        if \"images\" in dirs and (\"depth_metadata.json\" in files or \"metadata_temp\" in dirs):\n",
    "            if validate_root(p):\n",
    "                 print(f\"\u2705 Dataset extracted and verified at: {p}\")\n",
    "                 return p\n",
    "            \n",
    "    return None\n",
    "\n",
    "DATASET_PATH = setup_dataset()\n",
    "if not DATASET_PATH: raise RuntimeError(\"Dataset setup failed: Could not locate or reconstruct metadata\")\n",
    "DATASET_ROOT = DATASET_PATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the specific checkpoint (Hypersim, ViT-Base)\n",
    "CHECKPOINT_DIR = Path(\"checkpoints\")\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "ckpt_path = CHECKPOINT_DIR / \"depth_anything_v2_metric_hypersim_vitb.pth\"\n",
    "\n",
    "if not ckpt_path.exists():\n",
    "    print(\"\u2b07\ufe0f Downloading Checkpoint...\")\n",
    "    # Using typical HuggingFace link pattern for Depth Anything V2\n",
    "    url = \"https://huggingface.co/depth-anything/Depth-Anything-V2-Metric-Hypersim-Base/resolve/main/depth_anything_v2_metric_hypersim_vitb.pth\"\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    with open(ckpt_path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    print(\"\u2705 Checkpoint downloaded.\")\n",
    "else:\n",
    "    print(\"\u2705 Checkpoint already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Inference (Console Command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for inference\n",
    "IMG_PATH = str(DATASET_PATH / \"images\")\n",
    "OUT_DIR = \"metric_depth_vis\"\n",
    "\n",
    "# Ensure output directory is clean\n",
    "if os.path.exists(OUT_DIR):\n",
    "    shutil.rmtree(OUT_DIR)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Construct the command\n",
    "dataset_images_abs = os.path.abspath(IMG_PATH)\n",
    "out_dir_abs = os.path.abspath(OUT_DIR)\n",
    "ckpt_abs = os.path.abspath(ckpt_path)\n",
    "\n",
    "print(\"\ud83d\ude80 Running Inference...\")\n",
    "# Using --save-numpy to ensure we get raw metric depth if supported, otherwise falling back to png parsing\n",
    "\n",
    "cmd = (\n",
    "    f\"python Depth-Anything-V2/metric_depth/run.py \"\n",
    "    f\"--encoder vitb \"\n",
    "    f\"--load-from {ckpt_abs} \"\n",
    "    f\"--max-depth 20 \" # As per user example\n",
    "    f\"--img-path {dataset_images_abs} \"\n",
    "    f\"--outdir {out_dir_abs} \"\n",
    "    f\"--save-numpy \" # Adding this to ensure high precision for metrics\n",
    ")\n",
    "\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Calculation Function (Reused from UniDepth NB)\n",
    "def compute_metrics(pred, gt, mask):\n",
    "    \"\"\"\n",
    "    Computes standard depth estimation metrics.\n",
    "    pred, gt: numpy arrays (meters)\n",
    "    mask: boolean numpy array (valid pixels)\n",
    "    \"\"\"\n",
    "    pred = pred[mask]\n",
    "    gt = gt[mask]\n",
    "    \n",
    "    # Threshold: per-pixel max ratio\n",
    "    thresh = np.maximum((gt / pred), (pred / gt))\n",
    "    a1 = (thresh < 1.25).mean()      # delta1\n",
    "    a2 = (thresh < 1.25 ** 2).mean() # delta2\n",
    "    a3 = (thresh < 1.25 ** 3).mean() # delta3\n",
    "\n",
    "    # Error metrics\n",
    "    rms = (gt - pred) ** 2\n",
    "    rmse = np.sqrt(rms.mean())\n",
    "\n",
    "    rms_log = (np.log(gt) - np.log(pred)) ** 2\n",
    "    rmse_log = np.sqrt(rms_log.mean())\n",
    "\n",
    "    abs_rel = np.mean(np.abs(gt - pred) / gt)\n",
    "    sq_rel = np.mean(((gt - pred) ** 2) / gt)\n",
    "\n",
    "    return {\n",
    "        'a1': a1,\n",
    "        'a2': a2,\n",
    "        'a3': a3,\n",
    "        'rmse': rmse,\n",
    "        'rmse_log': rmse_log,\n",
    "        'abs_rel': abs_rel,\n",
    "        'sq_rel': sq_rel\n",
    "    }\n",
    "\n",
    "def plot_results(result_dir):\n",
    "    \"\"\"\n",
    "    Reads results from output dir, computes metrics using GT, and visualizes.\n",
    "    This mimics the user's requested style.\n",
    "    \"\"\"\n",
    "    metrics_list = []\n",
    "    result_path = Path(result_dir)\n",
    "    \n",
    "    if not result_path.exists():\n",
    "        print(f\"\u274c Result directory {result_dir} does not exist!\")\n",
    "        return\n",
    "\n",
    "    npy_files = sorted(list(result_path.glob(\"*.npy\")))\n",
    "    \n",
    "    if len(npy_files) > 0:\n",
    "        file_list = npy_files\n",
    "        is_npy = True\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f No .npy files found. Falling back to .png (less accurate)...\")\n",
    "        file_list = sorted(list(result_path.glob(\"*.png\")))\n",
    "        is_npy = False\n",
    "        \n",
    "    print(f\"Processing {len(file_list)} output files...\")\n",
    "    \n",
    "    last_pred = None\n",
    "    last_gt = None\n",
    "    last_metrics = None\n",
    "    last_stem = \"\"\n",
    "\n",
    "    for pred_path in tqdm(file_list):\n",
    "        if \"_raw\" in pred_path.stem:\n",
    "             stem = pred_path.stem.replace(\"_raw\", \"\")\n",
    "        else:\n",
    "             stem = pred_path.stem\n",
    "             \n",
    "        gt_path = DATASET_PATH / \"depth_npy\" / f\"{stem}.npy\"\n",
    "        if not gt_path.exists():\n",
    "             gt_path = DATASET_PATH / \"depth\" / f\"{stem}.npy\"\n",
    "        \n",
    "        if not gt_path.exists():\n",
    "            continue\n",
    "            \n",
    "        gt_depth = np.load(gt_path).squeeze()\n",
    "        \n",
    "        if is_npy:\n",
    "            pred_depth = np.load(pred_path)\n",
    "        else:\n",
    "            # Fallback for PNG (typically 16-bit uint mm)\n",
    "            img = cv2.imread(str(pred_path), cv2.IMREAD_UNCHANGED)\n",
    "            if img is None: continue\n",
    "            pred_depth = img.astype(float) / 1000.0\n",
    "            \n",
    "        # Resize if mismatch\n",
    "        if pred_depth.shape != gt_depth.shape:\n",
    "            pred_depth = cv2.resize(pred_depth, (gt_depth.shape[1], gt_depth.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "            \n",
    "        mask = (gt_depth > 0) & (gt_depth < 10)\n",
    "        if mask.sum() == 0: continue\n",
    "        \n",
    "        metrics = compute_metrics(pred_depth, gt_depth, mask)\n",
    "        metrics_list.append(metrics)\n",
    "        \n",
    "        last_pred = pred_depth\n",
    "        last_gt = gt_depth\n",
    "        last_metrics = metrics\n",
    "        last_stem = stem\n",
    "\n",
    "    # Aggregate\n",
    "    if metrics_list:\n",
    "        avg = {}\n",
    "        for k in metrics_list[0].keys():\n",
    "            avg[k] = np.mean([m[k] for m in metrics_list])\n",
    "            \n",
    "        print(\"\\n=== Evaluation Results ===\")\n",
    "        print(f\"Abs Rel:  {avg['abs_rel']:.4f}\")\n",
    "        print(f\"RMSE:     {avg['rmse']:.4f}\")\n",
    "        print(f\"RMSE log: {avg['rmse_log']:.4f}\")\n",
    "        print(f\"\u03b41 (Acc): {avg['a1']:.4f}\")\n",
    "        print(f\"Sq Rel:   {avg['sq_rel']:.4f}\")\n",
    "        \n",
    "        # Visualize\n",
    "        plt.figure(figsize=(20, 6))\n",
    "        \n",
    "        # RGB\n",
    "        rgb_path = DATASET_PATH / \"images\" / f\"{last_stem}.png\"\n",
    "        if rgb_path.exists():\n",
    "            plt.subplot(1,3,1)\n",
    "            plt.imshow(Image.open(rgb_path))\n",
    "            plt.title(\"RGB Input\")\n",
    "            plt.axis(\"off\")\n",
    "            \n",
    "        # GT\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(last_gt, cmap='magma', vmin=0, vmax=5)\n",
    "        plt.title(\"Ground Truth (.npy)\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.colorbar(label='Meters')\n",
    "        \n",
    "        # Pred\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(last_pred, cmap='magma', vmin=0, vmax=5)\n",
    "        plt.title(f\"Prediction (DAV2)\\nAbsRel: {last_metrics['abs_rel']:.3f}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.colorbar(label='Meters')\n",
    "        \n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No valid metrics computed.\")\n",
    "\n",
    "# Run the evaluation function\n",
    "plot_results(OUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}